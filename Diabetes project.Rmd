---
title: Extensive Guide for Health Care Data Analysis using R(Machine Learning Algorithms,
  GLM)
author: "ERICK@"
date: "2024-04"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## **This article is all about detailed Base Model analysis of the Diabetes Data which includes the following analysis:**

1.  Data exploration (Data distribution inferences, Univariate Data analysis, Two-sample t-test)

2.  Data Correlation Analysis

3.  Feature Selection (using Logistic regression)

4.  Outlier Detection (using principal component graph)

5.  Basic Parameter Tuning(CV, complexity parameter)

6.  Data modeling

Basic GLM (With all Features and eliminating few features based on AIC)

-   Logistic Regression

-   Decision Tree

-   Naïve Bayes

```{r warnings=F, message=F}
#import Libraries
library(bookdown)
library(tidyverse)
library(rmarkdown)
library(flexdashboard)
```

## 

```{r  warnings=F, message=F}
# {r warnings=F, message=F}
#IMPORT DATASET
diabetes <- read_csv("C:/Users/langa/OneDrive/Desktop/R PROGRAMMING PRACTICE/Extensive Guide for Health Care Data Analysis using R(Machine Learning Algorithms)#/diabetes.csv")
diabetes <- diabetes %>% 
     mutate(
       Outcome= ifelse(Outcome==0, 'NO','YES'), 
                    # base::as.factor(Outcome)
                           )

head(diabetes)
diabetes$Outcome <- as.factor(diabetes$Outcome)
# levels(diabetes$Outcome)
```

## Basic EDA

```{r  warnings=F, message=F}
summary(diabetes)

```

#### **Uni-variate analysis**

```{r  warnings=F, message=F}
#library(patchwork)
par(mfrow=c(2,2))
p1 <- hist(diabetes$Pregnancies)
p2 <- hist(diabetes$Glucose)
p3 <- hist(diabetes$BMI)
p4 <- hist(diabetes$Age)
```

From these distribution graphs, Age and number of times pregnant are not in normal distributions as expected since the underlying population should not be normally distributed either.

Glucose level and BMI are following a normal distribution.

```{r warning=FALSE, message=FALSE}
boxplot(diabetes$BloodPressure, ylab="BloodPressure")
```

### **Impact of Glucose on Diabetes**

```{r  warning=FALSE, message=FALSE}
ggplot(diabetes, aes(x=Glucose))+
     geom_histogram(fill='deepskyblue', col='red') +
      facet_grid(Outcome~.)
```

Formulate a hypothesis to assess the mean difference of glucose levels between the positive and negative groups.

**Conditions**

Individuals are independent of each other

Here distributions are skewed but the sample \>30

Both the groups are independent of each other and the sample size is lesser than 10% of the population,

```{r warning=FALSE, message=FALSE}
library(report)
t <- t.test(Glucose~Outcome, data = diabetes) 
t
report(t)
```

p-value is \< critical values of 0.05, so we reject the null hypothesis for the alternate hypothesis. We can say that we are, 95 % confident, that the average glucose levels for individuals with diabetes is \> the people without diabetes.

```{r warning=FALSE, message=FALSE}
t1 <- t.test(Age~Outcome, data = diabetes)
t1
report(t1)
```

p_value\<0.001, suggest that diabetic people is mostly old.

```{r}
theme_set(theme_test())
diabetes %>% 
  ggplot(aes(x=cut(Age, breaks = 5))) +
        geom_boxplot(aes(y=DiabetesPedigreeFunction), col='deepskyblue')+
  labs(
    x = "Age Breaks =5",
    y = "DiabetesPedigreeFunction",
    colour = " ",
    shape = " "
   ) 

```

### Insulin Vs Glucose based on Outcome as diabetes

```{r warning=FALSE, message=FALSE}
diabetes %>% 
  ggplot(aes(x=Insulin,y=Glucose)) +
    geom_point() +
  geom_smooth()
```

```{r warning=FALSE, message=FALSE}
# par(mfrow=c(1,2))
#boxplot
library(patchwork)
p0 <- diabetes %>% 
  ggplot(aes(x=DiabetesPedigreeFunction))+
  geom_boxplot() +
  facet_wrap(~Outcome) + coord_flip() +
  ggtitle('Boxplot')

p1 <- diabetes %>% 
  ggplot(aes(x=Glucose, col=Outcome))+
    geom_density()+
    facet_wrap(~Outcome) +
    ggtitle('Density plot of Glucose')

(p0 + p1)
```

From Density Plot the distribution is shifted towards the left for those without diabetes.

This indicates those **without diabetes generally have a lower blood glucose level**.

```{r warning=FALSE, message=FALSE}
#two sample t-test
t2 <- t.test(DiabetesPedigreeFunction~Outcome, data = diabetes) #%>% report()
t2
report(t2)
```

#### **Correlation between each variable**

Scatter matrix of all columns

```{r warning=FALSE, message=FALSE}
library(GGally)
diabetes %>% select(-Outcome) %>% 
  ggcorr(
    name = "corr", label = TRUE
  ) +
  theme(legend.position = 'none') +
  labs(title = 'Correlation Plot of Variance') +
  theme(plot.title = element_text(
    face = 'bold',                                color = 'deepskyblue',
    hjust = 0.5, size = 11)) 
```

Pregnancy, Age, Insulin, skinthickness are having higher correlation.

### **Fitting a logistic regression to assess importance of predictors**

-   Fitting a GLM (General Linear Model) with link function ‘probit’

-   Target variable ‘diabetes’ estimated to be binomially distributed

This is a generic implementation — without assumption on data

```{r warning=FALSE, message=FALSE}
logit <- glm(Outcome~.,, data = diabetes, family = binomial())
summary(logit)
report(logit)
```

#### Filtering the most important predictors from GLM model Extracting the N most important GLM coefficients

### Features Selection

-   Highest logistic model coefficients

```{r}
model_coef <- exp(coef(logit))[2:ncol(diabetes)]
model_coef <- model_coef[c(order(model_coef,decreasing = TRUE)[1:(ncol(diabetes)-1)])]
predictors_names <- c(names(model_coef), names(diabetes)[length(diabetes)])
predictors_names
```

```{r}
#filter df with most important predictors
diabetes_df <- diabetes[, c(predictors_names)]
head(diabetes_df)
```

### Outlier Detection

```{r warning=FALSE, message=FALSE}
library(DMwR2)
outlier_scores <- diabetes %>% select(-Outcome) %>% 
            lofactor(k=5)
plot(density(outlier_scores))
```

```{r warning=FALSE, message=FALSE}
outliers <- order(outlier_scores, 
                  decreasing = TRUE)[1:5]
print(outliers)
```

## **The five outliers obtained in the output are the row numbers in the diabetes1 data derived from the diabetes data set.**

```{r warning=FALSE, message=FALSE}
# #labels outliers
n <- nrow(diabetes)
labels <- 1:n
labels[-outliers] <- "."
biplot(prcomp(diabetes[,-9], na.rm=TRUE), cex=.8, xlabs=labels)
```

```{r warning=FALSE, message=FALSE}
library(Rlof)
outlier.scores <- lof(diabetes[,-9], k=5)
outlier.scores<-lof(diabetes[,-9],k=c(5:10))
outlier.scores %>% head(4)
```

## Data Modelling

```{r}
#data partitioning/Train and Test
library(rsample)
split <- initial_split(diabetes, prop = 8/10)
train <- training(split)
test <- testing(split)
```

1.  **Basic GLM with all Variables**

    ```{r warning=FALSE, message=FALSE}
    log <- glm(Outcome~., family = binomial(), data = train)
    summary(log)
    ```

The result shows that the variables Triceps_Skin, Serum_Insulin and Age are not statistically significant. p_values is \> 0.01 so we can experiment by removing it.

## **Logistic Model**

input: explanatory variables xk and provides a prediction p with parameters $βk$.

The logit transformation constrains the value of p to the interval [0, 1].

\#

βk represents the log-odds of feature xk says how much the logarithm of the odds of a positive outcome (i.e. the logit transform) increases when predictor xk increases by .

Likelihood of the model as follows:

\#

$Y\^i$ = outcome of subject i.

Maximizing the likelihood = maximizing the log-likelihood(model)

\#

The above equation is non-linear for logistic regression and its minimization is generally done numerically by iteratively re-weighted least-squares

```{r}
smodel <- step(log)
```

### **The final model is chosen with AIC as the selection generated from a logistic regression model with the lowest AIC value of 584.68.**

## **Initial Parameter Tuning**

```{r}
library(rpart)
library(rpart.plot)
tree <- rpart(Outcome~., method = 'class', data = diabetes)
rpart.plot(tree)
```

```{r}
plotcp(tree)
```

**Complexity parameter**

The above tree was tuned using a reference of the Relative error VS Complexity parameter. From the above figure the Cp value of 0.016, the decision tree was pruned. The final decision tree

```{r}
tree1 <- rpart(Outcome~., method = 'class', data = diabetes, cp=0.016)
rpart.plot(tree1)
```

If CP value is lower, tree will grow large. A cp = 1 will provide no tree which helps in pruning the tree. Higher complexity parameters can lead to an over pruned tree.

**2nd Model By removing 3 features-**

```{r}
log1 <- glm(Outcome~., family = binomial(), data=(train %>% select(-c(Age,SkinThickness, 
                BloodPressure ,                                         Insulin))))

summary(log1)
report(log1)
```

```{r}
par(mfrow=c(2,2))
plot(log1)
```

1\. **Residuals vs fitted values**; Here dotted line at y=0 indicates fit line. The points on fit line indication of zero residual. Points above are having positive residuals similarly points below have negative residuals. . The red line is indicates smoothed high order polynomial curve which provides the idea behind pattern of residual movements. Here the residuals have logarithmic pattern hence we got a good model.

2\. **Normal Q-Q Plot:** In general Normal Q-Q plot is used to check if our residuals follow Normal distribution or not. The residuals are said to be **normally distributed** if points follow the dotted line closely.

In our case residual points follow the dotted line closely except for observation at 229, 350 and 503 So this model residuals passed the test of Normality.

3\. **Scale — Location Plot:** It indicates spread of points across predicted values range.

Assumption:

\- Variance should be reasonably equal across the predictor range(Homoscedasticity)

So this horizontal red line is set to be ideal and it indicates that residuals have uniform variance across the Predictor range. As residuals spread wider from each other the red spread line goes up. In this case the data is Homoscedastic i.e having **uniform variance**.

4\. **Residuals vs Leverage Plot**:

**Influence**: The Influence of an observation can be defined in terms of how much the predicted scores would change if the observation is excluded. Cook’s Distance

**Leverage**: The leverage of an observation is defined on how much the observation’s value on the predictor variable differs from the mean of the predictor variable. **The more the leverage of an observation , the greater potential that point has in terms of influence**.

In our plot the dotted red lines are the cook’s distance and the areas of interest for us are the ones outside the dotted line on the top right corner or bottom right corner. If any point falls in that region, we say the observation has high leverage or having some potential for influencing our model is higher if we exclude that point.

**3rd Model: Predict Diabetes Risk on new patients using Decision Tree**

```{r}
library(party)
ct <- ctree(Outcome~.,data=train)
# plot(ct)
predict_clas <- predict(ct, test,
                         type=c('response'))
table(predict_clas, test$Outcome)
```

```{r}
library(caret)
con_mat <- confusionMatrix(test$Outcome, predict_clas, positive = NULL,
              dnn = c('Prediction', 'References'))
con_mat
```

### 4th Model Naïve Bayes:

```{r warning=FALSE, message=FALSE}
library(e1071)
nb <- naiveBayes(Outcome~., data = train)
pred <- predict(nb, test)
confusionMatrix(test$Outcome, pred)

```

**Though it’s a basic model still it performed well with 77% accuracy on an average**
